{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "file_save_h5 = 'SAVED H5PY FILE'\n",
    "\n",
    "with h5py.File(str(file_save_h5), \"r\") as hf:\n",
    "    X = hf[\"embeddings\"][:]\n",
    "    y = hf[\"labels\"][:]\n",
    "\n",
    "X_train, X_rem, y_train, y_rem = train_test_split(\n",
    "    X,\n",
    "    y, \n",
    "    train_size=0.8,\n",
    "    random_state=22,\n",
    ")\n",
    "\n",
    "test_size = 0.5\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_rem,y_rem, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autosklearn.regression\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "import sklearn.datasets\n",
    "import sklearn.metrics\n",
    "\n",
    "from smac.tae import StatusType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_runhistory_models_performance(automl):\n",
    "    metric = automl.automl_._metric\n",
    "    data = automl.automl_.runhistory_.data\n",
    "    performance_list = []\n",
    "    for run_key, run_value in data.items():\n",
    "        if run_value.status != StatusType.SUCCESS:\n",
    "            # Ignore crashed runs\n",
    "            continue\n",
    "        # Alternatively, it is possible to also obtain the start time with ``run_value.starttime``\n",
    "        endtime = pd.Timestamp(time.strftime('%Y-%m-%d %H:%M:%S',\n",
    "                                             time.localtime(run_value.endtime)))\n",
    "        val_score = metric._optimum - (metric._sign * run_value.cost)\n",
    "        test_score = metric._optimum - (metric._sign * run_value.additional_info['test_loss'])\n",
    "        train_score = metric._optimum - (metric._sign * run_value.additional_info['train_loss'])\n",
    "        performance_list.append({\n",
    "            'Timestamp': endtime,\n",
    "            'single_best_optimization_score': val_score,\n",
    "            'single_best_test_score': test_score,\n",
    "            'single_best_train_score': train_score,\n",
    "        })\n",
    "    return pd.DataFrame(performance_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr = 3600\n",
    "task_name = \"yield_TAPE_30min_ensemble2_80training_10test_10valid_noCV\"\n",
    "time_left_for_this_task_in_seconds = int(0.5 * hr)\n",
    "automl = autosklearn.regression.AutoSklearnRegressor(\n",
    "    time_left_for_this_task=time_left_for_this_task_in_seconds,\n",
    "    tmp_folder=f\"{task_name}/\",\n",
    "    n_jobs=-1,\n",
    "    memory_limit=None,\n",
    "    per_run_time_limit=60\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"automl = autosklearn.regression.AutoSklearnRegressor(\n",
    "    time_left_for_this_task=time_left_for_this_task_in_seconds,\n",
    "    tmp_folder=f\"{task_name}/\",\n",
    "    n_jobs=-1,\n",
    "    memory_limit=None,\n",
    "    resampling_strategy=\"cv\",\n",
    "    ensemble_size = 5,\n",
    "    resampling_strategy_arguments={\"folds\": 20},\n",
    "    include_preprocessors=[\n",
    "        \"no_preprocessing\",\n",
    "    ],\n",
    ")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl.fit(X_train, y_train, X_test=X_test, y_test=y_test, dataset_name=task_name)\n",
    "#automl.refit(X_train.copy(), y_train.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_performance_frame = pd.DataFrame(automl.automl_.ensemble_performance_history)\n",
    "best_values = pd.Series({'ensemble_optimization_score': -np.inf,\n",
    "                         'ensemble_test_score': -np.inf})\n",
    "for idx in ensemble_performance_frame.index:\n",
    "    if (\n",
    "        ensemble_performance_frame.loc[idx, 'ensemble_optimization_score']\n",
    "        > best_values['ensemble_optimization_score']\n",
    "    ):\n",
    "        best_values = ensemble_performance_frame.loc[idx]\n",
    "    ensemble_performance_frame.loc[idx] = best_values\n",
    "\n",
    "individual_performance_frame = get_runhistory_models_performance(automl)\n",
    "best_values = pd.Series({'single_best_optimization_score': -np.inf,\n",
    "                         'single_best_test_score': -np.inf,\n",
    "                         'single_best_train_score': -np.inf})\n",
    "for idx in individual_performance_frame.index:\n",
    "    if (\n",
    "        individual_performance_frame.loc[idx, 'single_best_optimization_score']\n",
    "        > best_values['single_best_optimization_score']\n",
    "    ):\n",
    "        best_values = individual_performance_frame.loc[idx]\n",
    "    individual_performance_frame.loc[idx] = best_values\n",
    "\n",
    "pd.merge(\n",
    "    ensemble_performance_frame,\n",
    "    individual_performance_frame,\n",
    "    on=\"Timestamp\", how='outer'\n",
    ").sort_values('Timestamp').fillna(method='ffill').plot(\n",
    "    x='Timestamp',\n",
    "    kind='line',\n",
    "    legend=True,\n",
    "    title='Auto-sklearn accuracy over time',\n",
    "    grid=True,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = automl.predict(X_train)\n",
    "print(\"Train R2 score:\", sklearn.metrics.r2_score(y_train, train_predictions))\n",
    "test_predictions = automl.predict(X_test)\n",
    "print(\"Test R2 score:\", sklearn.metrics.r2_score(y_test, test_predictions))\n",
    "valid_predictions = automl.predict(X_valid)\n",
    "print(\"Validation R2 score:\", sklearn.metrics.r2_score(y_valid, valid_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(train_predictions, y_train, label=\"Train\", c='#00BDE3')\n",
    "plt.scatter(test_predictions, y_test, label=\"Test\", c='#b734eb')\n",
    "plt.scatter(valid_predictions, y_valid, label=\"Valid\", c='#fc9003')\n",
    "plt.xlabel(\"Predicted yield (log(ug/ml))\")\n",
    "plt.ylabel(\"True yield (log(ug/ml))\")\n",
    "plt.legend()\n",
    "#plt.plot([1.5, 5.0], [0.5, 6.0], c='k', zorder=0)\n",
    "\n",
    "#plt.xlim([1.5, 5.0])\n",
    "#plt.ylim([0.5, 6.0])\n",
    "plt.tight_layout()\n",
    "plt.savefig('yield_log_autoML_best_ensemble_8hrs_ESM_ensemble5.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train correlation:', np.corrcoef(train_predictions, y_train)[0][1])\n",
    "print('Test correlation:', np.corrcoef(test_predictions, y_test)[0][1])\n",
    "print('Validation correlation:', np.corrcoef(valid_predictions, y_valid)[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "corr_train, _ = pearsonr(train_predictions, y_train)\n",
    "print('Pearsons correlation for training set: %.3f' % corr_train)\n",
    "corr_test, _ = pearsonr(test_predictions, y_test)\n",
    "print('Pearsons correlation for training set: %.3f' % corr_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl.fit_ensemble(y_train, ensemble_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = automl.predict(X_test)\n",
    "print(automl.sprint_statistics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model, and load afterwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "\n",
    "dump_file = 'property_best_ensemble_8hrs_ESM.pkl'\n",
    "\n",
    "with open(dump_file, 'wb') as f:\n",
    "    pkl.dump(automl, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'property_best_ensemble_8hrs_ESM.pkl'\n",
    "\n",
    "with open(dump_file, 'rb') as f:\n",
    "    restored_automl = pkl.load(f)\n",
    "\n",
    "restored_train_pred = restored_automl.predict(X_train)\n",
    "restored_test_pred = restored_automl.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_train, _ = pearsonr(restored_train_pred, y_train)\n",
    "print('Pearsons correlation for training set: %.3f' % corr_train)\n",
    "corr_test, _ = pearsonr(restored_test_pred, y_test)\n",
    "print('Pearsons correlation for training set: %.3f' % corr_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(restored_train_pred, y_train, label=\"Restored train samples\", c='#d95f02')\n",
    "plt.scatter(restored_test_pred, y_test, label=\"Restored test samples\", c='#7570b3')\n",
    "plt.xlabel(\"Predicted property\")\n",
    "plt.ylabel(\"True property\")\n",
    "plt.legend()\n",
    "plt.plot([1.75, 4], \n",
    "         [0.5, 5], \n",
    "         c='k', \n",
    "         zorder=0)\n",
    "\n",
    "plt.xlim([1.75, 4])\n",
    "plt.ylim([0.5, 5])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(restored_test_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
